{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leander/anaconda3/envs/pytorch/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#import os\n",
    "from mimic_data import load_dummy,load_merge\n",
    "from update_and_write import update_write,initial_tokenizer_text,initial_tokenizer_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leander/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Create_Folder(newpath):\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create needed folders\n",
    "Create_Folder(newpath='texts')\n",
    "Create_Folder(newpath='logs')\n",
    "Create_Folder(newpath='tf_rec')\n",
    "Create_Folder(newpath='data')\n",
    "Create_Folder(newpath='tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Set Up  Tokenizer\n"
     ]
    }
   ],
   "source": [
    "#Here we run the Initializer of the TOkenizer\n",
    "#RUN ONLY ONCE\n",
    "#Always save the tokenizers. it is needed in every step. \n",
    "#We init text tokenizer with ICD 9 Descriptions, all words present in the ICD descriptions will be in the tokenizer \n",
    "initial_tokenizer_text(tokenizer_name='tokenizer/Text_Tokenizer.pkl'\n",
    "                      ,ICD_file_en='data/icd9.txt')\n",
    "#We init label tokenizer empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sets up a tokenizer with \n",
    "initial_tokenizer_label(tokenizer_name='tokenizer/Label_Tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The text and label tokenizer which are to be updated\n",
    "#Also the location and name of the output. \n",
    "#Be careful not to name it like an already existing tf records cause it will \n",
    "#get overwritten \n",
    "\n",
    "text_tokenizer_path='tokenizer/Text_Tokenizer.pkl'\n",
    "label_tokenizer_path='tokenizer/Label_Tokenizer.pkl'\n",
    "output_name='tf_rec/MIMIC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For Loading the dummy dataset\n",
    "In_Text,labels=load_dummy()\n",
    "\n",
    "#For Loading the MIMIC dataset\n",
    "#In_Text,labels=load_merge(diag_path='../DIAGNOSES_ICD.csv', notes_path='../NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Notes\n",
      "Update Text Tokenizer\n",
      "Words added:\n",
      "23\n",
      "Update Label Tokenizer\n",
      "Empty dictionary: Initial Tokenizer ? \n",
      "Labels added:\n",
      "13597\n",
      "Encode the Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=                                                         ]   2% ETA:  0:00:07\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Tf Records Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[============                                              ]  21% ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Tf Records Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[=============                                             ]  23% ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Tf Records Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================= ]  99% ETA:  0:00:00\r"
     ]
    }
   ],
   "source": [
    "#Finally We call the \"update and write\" function. which:\n",
    "#1.) Cleanes the Notes. \n",
    "#2.) Updates the tokenizer\n",
    "#3.) Encodes the texts\n",
    "#4.) Does Train Validation Test splitting. \n",
    "#5.) writes 3 TF records at given output path (Train, Validation, Test)\n",
    "\n",
    "update_write(In_Text=In_Text,labels=labels,output_name=output_name,tokenizer_label_path=label_tokenizer_path,tokenizer_text_path=text_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorchenv)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
